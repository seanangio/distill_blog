[
  {
    "path": "posts/2022-09-01-functional-dependencies-and-relational-database-design/",
    "title": "Functional Dependencies and Relational Database Design",
    "description": "Determine functional dependencies to structure relations in third normal form.",
    "author": [],
    "date": "2022-09-01",
    "categories": [
      "SQL",
      "relational database theory"
    ],
    "contents": "\n\nContents\nFunctional Dependencies\nNormal Forms\nSpeeches\nExample\n\nI recently completed the Databases\nfor Data Scientists specialization on Coursera from the University\nof Colorado Boulder. Most of the three courses in the specialization\nwere a review for me, but the Relational\nDatabase Design course covered normalization theory in greater\ndetail than I had previously seen.\nAlthough I was familiar with entity relationship diagrams and the\nbenefits of normalization as far as minimizing data redundancy, I had\nnot studied the more formal normalization process and the concept of\nfunctional dependencies.\nFunctional Dependencies\nIn a relation (a table), an attribute (a column) such as\n“date_of_birth” would always determine a column “zodiac_sign”. In other\nwords, “zodiac_sign” is functionally dependent on “date_of_birth”.\nKnowing these kinds of relationships between attributes forms the basis\nfor which attributes belong in a relation and which should be linked\ntogether with foreign keys.\nNormal Forms\nPreviously, I’ve focused primarily on minimizing data redundancy when\ndetermining what attributes belong in the same relation. Having a more\nformal system of first, second, and third normal forms adds structure to\nthis process.\nIn particular, the course introduced:\npartial functional dependencies—and how to remove them to\nrestructure a table from first to second normal form.\ntransitive functional dependencies—and how to remove them in order\nto restructure a table from second to third normal form.\nSpeeches Example\nIn a previous\nproject, I created a normalized PostgreSQL database from a CSV file\nof India’s Independence Day speeches. Spurred on by what I learned in\nthe PostgreSQL for Everybody\nspecialization, my goal was to store every string only once and link\ntables together with integer keys.\nERD for speech databaseNow armed with the terminology of relational database theory, I would\nnot change the database’s structure, but I can provide new\njustifications for why the design is “suitable”.\nFor every relation, the primary key determines all of the other\nattributes. For example, the corpus relation, can be\nrepresented as:\ncorpus (year (pk), pm_id(fk), party_id(fk), title_id(fk), footnote_id(fk), source_id(fk), url_id(fk), text_id(fk))\nIt has only one functional dependency, which could be written as:\nyear -> pm_id, party_id, title_id, footnote_id, source_id, url_id, text_id\nOf course, this is an overly simple example, but the principles can\nbe useful for working out more complicated situations.\n\n\n\n",
    "preview": "posts/2022-09-01-functional-dependencies-and-relational-database-design/../2022-04-25-postgresql-database-design/img/featured.png",
    "last_modified": "2022-08-29T21:24:06-04:00",
    "input_file": "functional-dependencies-and-relational-database-design.knit.md",
    "preview_width": 3520,
    "preview_height": 1820
  },
  {
    "path": "posts/2022-05-01-text-data-in-postgresql/",
    "title": "Text Data in PostgreSQL",
    "description": "Use regular expressions and text analysis functions on India's Independence day speeches.",
    "author": [],
    "date": "2022-05-01",
    "categories": [
      "SQL",
      "PostgreSQL",
      "DBI",
      "RPostgres",
      "NLP"
    ],
    "contents": "\n\nContents\nFilter by regular expression\nFind speech lengths\nCreate one row per word\nFind the most frequent words\nFind distinct words\nCounts without stop words\nCount stemmed keywords\n\nAfter demonstrating the principles of database design in my last post, the next major focus of the PostgreSQL for Everybody specialization was working with text data, which is again convenient to explore with my dataset of Indian Independence day speeches.\nFilter by regular expression\nI had known about the LIKE operator to search for specified patterns in a column, but the specialization introduced how you can also just use regular expressions with the ~ operator.\nFor example, I could find all of the references to the US (using a regex to capture the most common spelling variations—while excluding the USSR from the results).\n\nSELECT year, pm.name,\n  regexp_matches(text.speech, \n  '(United States|United States of America|USA|U.S.A.|U\\.S\\.|US(?!SR))', \n  'g') AS us_occurrence\nFROM corpus\nJOIN pm ON corpus.pm_id = pm.id\nJOIN text ON corpus.text_id = text.id\nORDER BY year\nLIMIT 11;\n\nTable 1: Displaying records 1 - 10\nyear\nname\nus_occurrence\n1951\nJawaharlal Nehru\n{“United States of America”}\n1959\nJawaharlal Nehru\n{“United States”}\n1970\nIndira Gandhi\n{U.S.}\n1994\nP.V. Narasimha Rao\n{USA}\n2003\nA.B. Vajpayee\n{US}\n2005\nManmohan Singh\n{“United States”}\n2005\nManmohan Singh\n{“United States”}\n2006\nManmohan Singh\n{“United States of America”}\n2009\nManmohan Singh\n{“United States”}\n2015\nNarendra Modi\n{“United States”}\n\n\nThe US actually only comes up in 10 speeches—most consistently during the Manmohan Singh era.\nFind speech lengths\nIf I wanted to do some text analysis on the speeches, the length of speeches might be a basic place to start. char_length() returns the number of characters in a string.\n\n-- the longest speeches by character length\nSELECT year, pm.name AS pm, party.name AS party, \n  char_length(text.speech) AS speech_characters\nFROM corpus\nJOIN pm ON corpus.pm_id = pm.id\nJOIN party ON corpus.party_id = party.id\nJOIN text on corpus.text_id = text.id\nORDER BY char_length(text.speech) DESC\nLIMIT 5;\n\nTable 2: 5 records\nyear\npm\nparty\nspeech_characters\n2016\nNarendra Modi\nBJP\n62178\n2019\nNarendra Modi\nBJP\n52982\n2020\nNarendra Modi\nBJP\n50356\n2015\nNarendra Modi\nBJP\n49399\n2018\nNarendra Modi\nBJP\n47444\n\n\nAll of the longest speeches come from Narendra Modi. Instead of character lengths though, I’d usually be more interested in counting words, which requires a different table structure.\nCreate one row per word\nI’ve previously done this kind of basic text analysis work in R using the {tidytext} package. However, the third course in the specialization demonstrated how PostgreSQL can handle routine NLP tasks like tokenization, stemming, and removing stopwords.\nInstead of using tidytext::unnest_tokens() to produce a table of one token per row, I used Postgres’ string_to_array() and unnest() functions, along with a lateral join to split the speeches on white space and create one row per word.\n\n-- https://stackoverflow.com/questions/29419993/split-column-into-multiple-rows-in-postgres\nSELECT c.year, s.word\nFROM corpus c\nJOIN text t ON c.text_id = t.id\nCROSS JOIN LATERAL unnest(string_to_array(t.speech, ' ')) AS s(word)\nORDER BY c.year\nLIMIT 5;\n\nTable 3: 5 records\nyear\nword\n1947\nit\n1947\nhas\n1947\nFellow\n1947\ncountrymen,\n1947\nbeen\n\n\nFind the most frequent words\nI can then use a simple group to find the most frequent words in the entire speech, or change the GROUP BY clause to find the most frequent words per speech.\n\n-- most popular words in entire corpus\nSELECT s.token, COUNT(*)\nFROM corpus c\nJOIN text t\nON c.text_id = t.id\nCROSS JOIN LATERAL unnest(string_to_array(t.speech, ' ')) AS s(token)\nGROUP BY s.token\nORDER BY COUNT(*) DESC\nLIMIT 5;\n\nTable 4: 5 records\ntoken\ncount\nthe\n11971\nof\n8325\nto\n7954\nand\n6974\n\n6624\n\n\nFind distinct words\nWithout any basic preprocessing, this isn’t too interesting yet. To start, I first need to convert all text to lowercase; remove stop words; and apply stemming depending on the exact purpose.\nI can see that the entire corpus of speeches contains about 250k words, but only about 32k are distinct.\n\n-- total words in corpus\nSELECT COUNT(*) AS total_words, \n  COUNT(DISTINCT s.token) AS distinct_words\nFROM corpus c\nJOIN text t\nON c.text_id = t.id\nCROSS JOIN LATERAL unnest(string_to_array(LOWER(t.speech), ' ')) AS s(token);\n\nTable 5: 1 records\ntotal_words\ndistinct_words\n251971\n32204\n\n\nThis difference, as explained in the specialization, is one reason you don’t want to store every word in your index!\nCounts without stop words\nEven these distinct words though include many words that don’t contain any meaning. For that, I need to remove stop words. PostgreSQL has built-in functions for all of this, but I can do it manually. First creating the table structure:\n\nDROP TABLE IF EXISTS stop_words;\nCREATE TABLE IF NOT EXISTS stop_words (word TEXT unique);\n\nAnd then copying in a simple list found on GitHub.\n\n-- wget https://raw.githubusercontent.com/h2oai/h2o-tutorials/master/h2o-world-2017/nlp/stopwords.csv\n\\copy stop_words (word) FROM 'stopwords.csv' WITH DELIMITER ',' CSV HEADER;\n\nI then can filter out stop words with a subquery before counting word frequencies again. Now it returns words of some value (though the list of stop words could be expanded).\n\nSELECT s.word, \n  COUNT(s.word)\nFROM corpus c\nJOIN text t\nON c.text_id = t.id\nCROSS JOIN LATERAL unnest(string_to_array(LOWER(t.speech), ' ')) AS s(word)\nWHERE s.word NOT IN (SELECT word FROM stop_words)\nGROUP BY s.word\nORDER BY COUNT(s.word) DESC\nLIMIT 10;\n\nTable 6: Displaying records 1 - 10\nword\ncount\n\n6624\npeople\n1003\ncountry\n917\nindia\n902\nalso\n746\nus\n659\nnew\n625\nwould\n600\none\n564\nwant\n455\n\n\n*I’m actually not sure why an empty space is the most frequent word…\nCount stemmed keywords\nAnother step forward would be to apply stemming to the keywords before counting them. This is a particularly important step if creating an index.\nThe specialization showed how to create a kind of stem-to-keyword dictionary to demonstrate how it works. But to implement it on my own, it’s obviously much better to use the built-in features of PostgreSQL.\nPostgreSQL has a function to_tsvector() that turns a text string (like a speech) into an array that contains natural language features like stemming, stop words removal, etc.\nI’ll use this function to create a list of words that represent the speech document, and then apply ts_stat() to extract some information, like the most common keywords.\n\nSELECT * FROM ts_stat(\n  $$SELECT to_tsvector('english', t.speech) \n  FROM corpus c\n  JOIN text t\n  ON c.text_id = t.id$$)\nORDER BY nentry DESC, ndoc DESC, word\nLIMIT 10;\n\nTable 7: Displaying records 1 - 10\nword\nndoc\nnentry\ncountri\n73\n2191\nindia\n73\n1753\npeopl\n73\n1548\nyear\n72\n1112\nus\n73\n993\nalso\n71\n922\nnation\n73\n869\ngovern\n66\n859\ntoday\n73\n844\nnew\n67\n781\n\n\nNot surprisingly, patriotic words stems referring to “country”, “india”, “people”, and “nation” are the most common.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-05-01T18:44:44-04:00",
    "input_file": "text-data-in-postgresql.knit.md"
  },
  {
    "path": "posts/2022-04-25-postgresql-database-design/",
    "title": "PostgreSQL Database Design",
    "description": "Create a normalized SQL database from a CSV file of India's Independence Day speeches.",
    "author": [],
    "date": "2022-04-25",
    "categories": [
      "SQL",
      "PostgreSQL",
      "DBI",
      "RPostgres"
    ],
    "contents": "\n\nContents\nMake a Connection\nNormalize a Database\nCreate a raw table\nCopy CSV file into the raw table\nHandle missing data\nInsert distinct data into separate tables\nUpdate the raw table\nMake a new table of only foreign keys\n\nCreate an entity-relationship diagram\nTest some queries\nWrap-up\n\nI recently completed the PostgreSQL for Everybody specialization on Coursera from the University of Michigan.\nThis post builds on knowledge from the first two courses in the series, Database Design and Basic SQL in PostgreSQL and Intermediate PostgreSQL. I’ll demonstrate how to:\nstructure a CSV file into relational tables without vertical replication;\nuse the normalized database for some basic queries.\nI previously collected India’s Independence day speeches for an R data package, and included it as a CSV file, and so I can use that for demonstration.\nMake a Connection\nThe specialization focused on using psql or Python as the client to send commands to the PostgreSQL server. Here though I’ll use R as the client (and later pgAdmin).\n\n\ncon <- DBI::dbConnect(RPostgres::Postgres(),\n                      dbname = Sys.getenv(\"AUG15_dbname\"),\n                      host = Sys.getenv(\"AUG15_host\"),\n                      user = Sys.getenv(\"AUG15_user\"),\n                      password = Sys.getenv(\"AUG15_password\"),\n                      port = Sys.getenv(\"AUG15_port\"))\n\n\n\nNormalize a Database\nThe specialization discussed what it means to normalize a database, why that is important, and how to do it starting from a CSV file that has vertical replication of string data.\nCreate a raw table\nMy first step was to create an empty table in the database matching the schema of the CSV—with the addition of an \"_id\" integer column for every text column.\n\nDROP TABLE IF EXISTS corpus_raw;\nCREATE TABLE corpus_raw\n   (year INTEGER, \n   pm TEXT, pm_id INTEGER, \n   party TEXT, party_id INTEGER, \n   title TEXT, title_id INTEGER, \n   footnote TEXT, footnote_id INTEGER, \n   source TEXT, source_id INTEGER, \n   url TEXT, url_id INTEGER, \n   text TEXT, text_id INTEGER);\n\nCopy CSV file into the raw table\nOnce I made the empty raw table, I copied in the data from the CSV file with psql.\n\n-- wget https://raw.githubusercontent.com/seanangio/aug15/main/inst/final_csv/corpus.csv\n\\copy corpus_raw (year, pm, party, title, footnote, source, url, text) \nFROM 'corpus.csv' WITH DELIMITER ',' CSV HEADER;\n\nHandle missing data\nBefore doing anything though, I want to handle the missing data values.\nR codes missing data as NA, and so when I copied the CSV file into PostgreSQL tables it imported these values as the string “NA”. SQL though represents missing values with the value NULL, and so I should recode tables with missing values accordingly.\n\nUPDATE corpus_raw\n  SET title = (CASE WHEN title = 'NA' THEN NULL ELSE title END),\n      footnote = (CASE WHEN footnote = 'NA' THEN NULL ELSE footnote END),\n      source = (CASE WHEN source = 'NA' THEN NULL ELSE source END),\n      url = (CASE WHEN url = 'NA' THEN NULL ELSE url END),\n      text = (CASE WHEN text = 'NA' THEN NULL ELSE text END);\n\nNow with null values representing all of the missing data, I can continue.\nInsert distinct data into separate tables\nFrom the raw table, I then extracted the distinct data for every text column into its own table. This let me store exactly one copy of every string.\n\nDROP TABLE IF EXISTS party CASCADE;\nCREATE TABLE party (\n  id SERIAL,\n  name VARCHAR(128) UNIQUE,\n  PRIMARY KEY(id)\n);\n\nINSERT INTO party (name) \n  SELECT DISTINCT party \n  FROM corpus_raw \n  ORDER BY party;\n\nDROP TABLE IF EXISTS pm CASCADE;\nCREATE TABLE pm (\n  id SERIAL,\n  name VARCHAR(128),\n  PRIMARY KEY(id)\n);\n\nINSERT INTO pm (name)\n  SELECT DISTINCT pm\n  FROM corpus_raw \n  ORDER BY pm;\n\nDROP TABLE IF EXISTS title CASCADE;\nCREATE TABLE title (\n  id SERIAL,\n  title VARCHAR(128),\n  PRIMARY KEY(id)\n);\n\nINSERT INTO title (title) \n  SELECT DISTINCT title \n  FROM corpus_raw \n  ORDER BY title;\n\nDROP TABLE IF EXISTS footnote CASCADE;\nCREATE TABLE footnote (\n  id SERIAL,\n  footnote VARCHAR(1056),\n  PRIMARY KEY(id)\n);\n\nINSERT INTO footnote (footnote) \n  SELECT DISTINCT footnote \n  FROM corpus_raw \n  ORDER BY footnote;\n\nDROP TABLE IF EXISTS source CASCADE;\nCREATE TABLE source (\n  id SERIAL,\n  source VARCHAR(1056),\n  PRIMARY KEY(id)\n);\n\nINSERT INTO source (source) \n  SELECT DISTINCT source \n  FROM corpus_raw \n  ORDER BY source;\n\nDROP TABLE IF EXISTS url CASCADE;\nCREATE TABLE url (\n  id SERIAL,\n  url VARCHAR(1056),\n  PRIMARY KEY(id)\n);\n\nINSERT INTO url (url) \n  SELECT DISTINCT url \n  FROM corpus_raw \n  ORDER BY url;\n\nDROP TABLE IF EXISTS text CASCADE;\nCREATE TABLE text (\n  id SERIAL,\n  speech TEXT,\n  PRIMARY KEY(id)\n);\n\nINSERT INTO text (speech) \n  SELECT DISTINCT text \n  FROM corpus_raw \n  ORDER BY text;\n\nNow every table, such as party below, has an integer ID for every unique string, which is stored in the database only once.\n\nSELECT * FROM party;\n\nTable 1: 4 records\nid\nname\n1\nBJP\n2\nINC\n3\nJanata Dal\n4\nJanata Party\n\n\nUpdate the raw table\nI then inserted these integer keys into the raw table.\n\nUPDATE corpus_raw SET party_id = (SELECT party.id FROM party WHERE party.name = corpus_raw.party);\n\nUPDATE corpus_raw SET pm_id = (SELECT pm.id FROM pm WHERE pm.name = corpus_raw.pm);\n\nUPDATE corpus_raw SET title_id = (SELECT title.id FROM title WHERE title.title = corpus_raw.title);\n\nUPDATE corpus_raw SET footnote_id = (SELECT footnote.id FROM footnote \n    WHERE footnote.footnote = corpus_raw.footnote);\n\nUPDATE corpus_raw SET source_id = (SELECT source.id FROM source WHERE source.source = corpus_raw.source);\n\nUPDATE corpus_raw SET url_id = (SELECT url.id FROM url WHERE url.url = corpus_raw.url);\n\nUPDATE corpus_raw SET text_id = (SELECT text.id FROM text WHERE text.speech = corpus_raw.text);\n\nMake a new table of only foreign keys\nI then made a copy of the raw table, removed the redundant text columns, and added the foreign key constraints.\n\n-- copy to a new table\nDROP TABLE IF EXISTS corpus;\nCREATE TABLE corpus AS \nTABLE corpus_raw;\n\n-- drop un-normalized redundant text columns\nALTER TABLE corpus\nDROP COLUMN party,\nDROP COLUMN pm,\nDROP COLUMN title,\nDROP COLUMN footnote,\nDROP COLUMN source,\nDROP COLUMN url,\nDROP COLUMN text;\n\n-- add foreign key constraints\nALTER TABLE corpus\nADD CONSTRAINT fk_pm\nFOREIGN KEY (pm_id) \nREFERENCES pm (id);\n\nALTER TABLE corpus\nADD CONSTRAINT fk_party\nFOREIGN KEY (party_id) \nREFERENCES party (id);\n\nALTER TABLE corpus\nADD CONSTRAINT fk_title\nFOREIGN KEY (title_id) \nREFERENCES title (id);\n\nALTER TABLE corpus\nADD CONSTRAINT fk_footnote\nFOREIGN KEY (footnote_id) \nREFERENCES footnote (id);\n\nALTER TABLE corpus\nADD CONSTRAINT fk_source\nFOREIGN KEY (source_id) \nREFERENCES source (id);\n\nALTER TABLE corpus\nADD CONSTRAINT fk_url\nFOREIGN KEY (url_id) \nREFERENCES url (id);\n\nALTER TABLE corpus\nADD CONSTRAINT fk_text\nFOREIGN KEY (text_id) \nREFERENCES text (id);\n\nCreate an entity-relationship diagram\nI also loaded all of this into pgAdmin as one easy way to create an ERD.\nERD for aug15 databaseTest some queries\nMy last step was to confirm the results with some test queries. The new corpus table has only integer columns (which refer to strings in other tables).\n\nSELECT * FROM corpus ORDER BY year LIMIT 5;\n\nTable 2: 5 records\nyear\npm_id\nparty_id\ntitle_id\nfootnote_id\nsource_id\nurl_id\ntext_id\n1947\n6\n2\n11\n2\n2\nNA\n17\n1948\n6\n2\n39\n3\n2\nNA\n16\n1949\n6\n2\n43\n61\n3\nNA\n51\n1950\n6\n2\n44\n54\n22\n17\n52\n1951\n6\n2\n13\n57\n23\n18\n19\n\n\nI can recreate the original table through a series of joins. For example, I can find which prime ministers have given the most speeches.\n\nSELECT pm.name AS pm, \n    party.name AS party, \n    COUNT(*) AS speech_count\nFROM corpus\nJOIN pm ON corpus.pm_id = pm.id\nJOIN party ON corpus.party_id = party.id\nGROUP BY pm.name, party.name\nORDER BY COUNT(*) DESC\nLIMIT 10;\n\nTable 3: Displaying records 1 - 10\npm\nparty\nspeech_count\nJawaharlal Nehru\nINC\n17\nIndira Gandhi\nINC\n16\nManmohan Singh\nINC\n10\nNarendra Modi\nBJP\n8\nA.B. Vajpayee\nBJP\n6\nP.V. Narasimha Rao\nINC\n5\nRajiv Gandhi\nINC\n5\nMorarji Desai\nJanata Party\n2\nL.B. Shastri\nINC\n2\nI.K. Gujral\nJanata Dal\n1\n\n\n*(This assumes though Nehru gave a speech in 1962, which I’ve never found evidence of having taken place).\nThe focus of the specialization was not writing SQL itself, but it did show some techniques, such as for working with dates and casting columns. For example, I calculated the number of days since every speech.\n\nSELECT year, \n    EXTRACT(DAY FROM (\n        NOW() - (year || '-08-15')::date\n    )) AS days_since_speech\nFROM corpus \nORDER BY year DESC \nLIMIT 5;\n\nTable 4: 5 records\nyear\ndays_since_speech\n2021\n259\n2020\n624\n2019\n990\n2018\n1355\n2017\n1720\n\n\nWrap-up\nAnd so although a simple example, those are the basics of database design!\nIn future posts, I’ll use the same database to explore other topics in the specialization, such as regular expressions, indexes, and JSON.\n\n\n\n",
    "preview": "posts/2022-04-25-postgresql-database-design/img/featured.png",
    "last_modified": "2022-05-01T18:33:41-04:00",
    "input_file": "postgresql-database-design.knit.md",
    "preview_width": 3520,
    "preview_height": 1820
  },
  {
    "path": "posts/2021-09-05-an-r-data-package-for-indian-independence-day-speeches/",
    "title": "An R Data Package for Indian Independence Day Speeches",
    "description": "An R package including a dataset of full-text English renderings of Indian Independence Day speeches, delivered annually on 15 August since 1947.",
    "author": [],
    "date": "2021-09-05",
    "categories": [
      "NLP",
      "open-data",
      "r-packages",
      "shiny"
    ],
    "contents": "\nWhile living in Delhi, I made trips to various libraries to access the English renderings of Indian Independence Day speeches in the volumes of collected speeches from every Prime Minister. I digitized them and collected the results, along with those already accessible, into an R package.\nSince 1947, I’m only missing two years.\nYou can find the details of what’s included, how to install it (or just download the final CSV), some basic analysis, and a shiny app in the package’s GitHub repository.\n\n\n\n",
    "preview": "posts/2021-09-05-an-r-data-package-for-indian-independence-day-speeches/img/featured.png",
    "last_modified": "2021-12-22T20:47:22-05:00",
    "input_file": {},
    "preview_width": 2160,
    "preview_height": 1440
  },
  {
    "path": "posts/2021-07-07-an-r-data-package-for-preparing-and-visualizing-india-tagged-data-from-the-nyt-article-search-api/",
    "title": "An R Data Package for Preparing and Visualizing India-tagged Data from the NYT Article Search API",
    "description": "An R package to query the Article Search API of The New York Times for articles with an “India” location keyword. It also includes functions to prepare this data to be ready for analysis, as well as a shiny app to visualize the output dataset.",
    "author": [],
    "date": "2021-07-07",
    "categories": [
      "open-data",
      "r-packages",
      "shiny",
      "leaflet",
      "tidyverse",
      "ggiraph",
      "ggplot2",
      "dygraphs",
      "DT",
      "gt"
    ],
    "contents": "\nIn 2020, I started exploring what kind of data that the New York Times’ Article Search API was capable of returning. I began collecting all of the data with an India location keyword since the earliest available date (1851).\nWhat started as a data analysis project, I gradually reworked into an R package to make it easier to query the API every month and rebuild the cleaned output dataset fed into the shiny app’s visualizations.\nYou can find all of the package details, including vignettes, in the GitHub repository.\n\n\n\n",
    "preview": "posts/2021-07-07-an-r-data-package-for-preparing-and-visualizing-india-tagged-data-from-the-nyt-article-search-api/img/featured.png",
    "last_modified": "2021-12-22T20:46:53-05:00",
    "input_file": {},
    "preview_width": 2880,
    "preview_height": 1604
  },
  {
    "path": "posts/2019-07-02-working-with-databases-in-r/",
    "title": "Working with Databases in R",
    "description": "Use R to structure and query database tables of Indian census data",
    "author": [],
    "date": "2019-07-02",
    "categories": [
      "SQL",
      "dbplyr",
      "sf",
      "PostGIS",
      "rpostgis"
    ],
    "contents": "\n\nContents\nCreate a database\nWriting data from R to a PostgreSQL database\nWriting spatial data to a PostGIS-enabled PostgreSQL database\nQuerying SQL databases from R\nFinal Word and Further Resources\n\nMy recent blog posts have focused extensively on wrangling and visualizing data with R packages such as the {tidyverse}, {sf}, and {shiny}.\nHowever, as the size of data grows beyond what can be stored in one’s local memory, along with a variety of other possible reasons, a relational database becomes more of a necessity. With this in mind, I have been exploring PostgreSQL, including how to access SQL databases without leaving the familiar confines of RStudio.\nPreviously, I wrangled together three decades of district-level Indian electricity and latrine access data. I used this dataset to:\nhighlight the wide district-level distribution behind any country-level statistic with a choropleth and histogram;\ncompare mapping styles such as bubble maps, dot density maps, and 3D choropleths;\ncreate other visualizations that reveal the co-varying relationship between electricity and latrine access, such as a scatterplot and a bivariate choropleth.\nIn this post, I’ll demonstrate how I packaged this data into a PostGIS-enabled PostgreSQL database and how it can be queried from R. Specifically, this post covers:\nConnecting to a SQL database from R\nWriting data, including spatial data, to a remote database from R\nQuerying SQL databases from RStudio using SQL or {dplyr} syntax\nIn order to review how I created the R dataframes that I will write to the PostgreSQL database, please first examine this script here.\nMy previous dataframes were manipulated in such as way as to be prepared for their respective {shiny} applications. Thinking however of the most logical table structure for a database, I applied the following two steps. I divided country, state and district level data into separate tables. Moreover, I separated census data from spatial data.\nAccordingly, I devised a database with three spatial tables (district and state shapes, plus another for the dot density map), three tables of census data (district, state and country-level), and one additional table of some useful state-identifying information such as state abbreviations, region, and union territory status. From these tables, I can use any combination of joins to create the data for any of the previous {shiny} apps.\nCreate a database\nThis blog among other resources suggest it is possible to create a new SQLite database directly from R. However, that does not appear to be as simple with PostgreSQL. As a rarer task, I’ll just create a locally-hosted database from pgAdmin, the GUI for PostgreSQL with a simple one line command.\nCREATE DATABASE in_household;\nCreate a database connection\nWith the database initialized, I can start creating tables after establishing a connection. One can do this in R with the help of the {DBI} package.\nThe {DBI} package serves as a generic database interface for R. Packages like {RPostgres}, {RSQLite} or {RMySQL} then are implementations to forge connections between R and a particular database through the DBI interface.\nFor connecting to PostgreSQL from R, there are two possible implementation packages: {RPostgreSQL} and {RPostgres}. {Rpostgres} is newer and under more active development. According to this blog, it is also faster. However, when creating a connection via {RPostgres}, I had some trouble working with spatial data. This may be because I seem to be missing the correct driver. Nevertheless, there is also an {rpostgis} package with some handy functions that depends on the older {RPostgreSQL}. As speed won’t be a concern for me here, I’ll use the older {RPostgreSQL} implementation.\nIn the dbConnect() function from the {DBI} package, I specify PostgreSQL as the driver and give the database name. In many cases, here is where the user name and passwords would also be supplied.\n\n\nlibrary(DBI)\nlibrary(RPostgreSQL)\nlibrary(rpostgis)\nlibrary(dbplyr)\nlibrary(dplyr)\nlibrary(sf)\nlibrary(ggplot2)\n\ncon <- dbConnect(drv = \"PostgreSQL\", host = \"localhost\", \n                  dbname = \"in_household\")\n\nclass(con)\n\n\n[1] \"PostgreSQLConnection\"\nattr(,\"package\")\n[1] \"RPostgreSQL\"\n\nWriting data from R to a PostgreSQL database\nHaving connected to a database, I can now start writing data to it. I’ll start with the non-spatial data, demonstrating three methods.\nThe first writes a csv file through SQL code written in an R Markdown chunk, where SQL has been specified as the language engine.\nThe second directly writes an R dataframe through the DBI::dbWriteTable() function.\nThe third is a similar approach, but uses {dplyr}’s copy_to().\nSQL’s CREATE TABLE and COPY statements\nIn an R Markdown document, I can change the language engine from the default of R to another language, such as SQL. I then just need to specify the existing connection made through the {DBI} package. The result below is the same as if entering the query directly in pgAdmin.\nI do this in two stages. First I need a CREATE TABLE statement to initiate the table fields and associated constraints.\n\nCREATE TABLE IF NOT EXISTS states_abb_region_ut (\n    state varchar(90),                  -- state name,\n    abb char(2) NOT NULL,               -- state abbreviation\n    region varchar(90) NOT NULL,        -- state region\n    year date CHECK \n        ((year IN ('1991-01-01',\n        '2001-01-01','2011-01-01'))),    -- state year (states change over time)\n    ut_status boolean NOT NULL,         -- is it a union territory?\n        \n    CONSTRAINT state_key PRIMARY KEY (state, year)\n);\n\nThen I use a COPY statement to import the data with a path to a local csv file.\n\nCOPY states_abb_region_ut\nFROM '/path/to/data/states_abb_region_ut.csv'\nWITH (FORMAT CSV, HEADER);\n\nWith {DBI}\nRather than first writing an R object to a csv file, I can skip this intermediary step and, as shown below, directly write an R dataframe into a table in a remote database using the DBI::dbWriteTable() function.\n\n\n# district_census is a dataframe in R\nif (dbExistsTable(con, \"district_census\"))\n    dbRemoveTable(con, \"district_census\")\n\n# Write the data frame to the database\ndbWriteTable(con, name = \"district_census\", value = district_census, \n             row.names = FALSE)\n\n\n\nOn exporting the data, SQL has made its best guess at the data type of each column. It recognized year as a date, and the count data as integers. It assigned non-numeric data as text and decimal numbers as real numbers.\nIf I want to alter the table, such as changing data types or assigning constraints like primary keys or NOT NULL requirements, I can do that through the dbExecute() function. In this case, the primary key for the “district_census” table is the unique combination of seven variables.\n\n\ndbExecute(con,\n          \"ALTER TABLE district_census \n                ADD CONSTRAINT district_census_key PRIMARY KEY \n                    (district, state, year, societal_section, \n                    demo_section, water_source, water_avail);\")\n\n\n\nI can change data types with a command like this:\n\n\ndbExecute(con,\n          \"ALTER TABLE district_census\n                ALTER COLUMN geo_section TYPE char(8),\n                ALTER COLUMN state TYPE varchar(90),\n                ALTER COLUMN societal_section TYPE varchar(3),\n                ALTER COLUMN demo_section TYPE char(5),\n                ALTER COLUMN water_source TYPE varchar(90),\n                ALTER COLUMN water_avail TYPE varchar(90),\n                ALTER COLUMN district TYPE varchar(90);\")\n\n\n\nI could also use the {purrr} package to add constraints for columns requiring the same restrictions.\n\n\npercentages <- c(\"ea\",\"la\",\"ea_la\",\"ea_ln\",\"en_la\",\"en_ln\",\"within\",\"near\",\n                 \"away\",\"tap_treated\",\"tap_untreated\",\"covered_well\",\n                 \"uncovered_well\",\"hand_pump\",\"tube_well\",\"others\")\n\nwalk(percentages, ~ {\n    dbExecute(con,\n              paste0(\"ALTER TABLE district_census ADD CONSTRAINT \", .x, \n                     \"_check CHECK (\", .x, \" >= 0 AND \", .x, \" <= 1 OR NULL);\")\n    )\n})\n\n\n\nLet’s check if adding these constraints and key worked:\n\nSELECT constraint_name, constraint_type\nFROM information_schema.table_constraints\nWHERE table_name = 'district_census';\n\nTable 1: Displaying records 1 - 10\nconstraint_name\nconstraint_type\nea_check\nCHECK\nla_check\nCHECK\nea_la_check\nCHECK\nea_ln_check\nCHECK\nen_la_check\nCHECK\nen_ln_check\nCHECK\nwithin_check\nCHECK\nnear_check\nCHECK\naway_check\nCHECK\ntap_treated_check\nCHECK\n\n\nIt shows that the primary key, checks on percentage columns, and not null columns have been successfully added to the table.\nWith {dplyr}\nSimilar to {DBI}’s dbWriteTable(), I can also directly write R dataframes to a remote database with {dplyr}.\n{dplyr} is of course well-known as the {tidyverse}’s workhorse package for manipulation of data stored in local memory. However, with the help of {dbplyr}, it can also be used to manipulate data stored in remote databases.\n{dplyr}’s copy_to() function, as explained in the {dbplyr} vignette, is a quick and dirty way to write a dataframe to a remote database. The command below adds the “state_census” dataframe to the database.\n\n\ncopy_to(con, state_census, \"state_census\", temporary = FALSE)\n\n\n\nAs with the dbWriteTable() method, I would need to subsequently alter any data types, keys and other constraints.\nBecause I had already written the necessary SQL import statements, I’ll use the first method to import the remaining table (country_census). This code can be found in the “write_tables_sql” folder.\nWriting spatial data to a PostGIS-enabled PostgreSQL database\nWith the census data in place, I will now write the spatial data from R to the database. Doing so requires first installing the PostGIS extension. PostGIS is a spatial database extender for PostgreSQL. If you are familiar with R’s {sf} package, PostGIS will feel familiar– down to functions beginning with “ST_”.\nOne line of SQL code installs PostGIS.\n\nCREATE EXTENSION postgis;\n\nI can confirm the installation has been succesful by checking the version number. (I’ll explain the dbGetQuery() function in a later section).\n\n\ndbGetQuery(con, \"SELECT postgis_full_version();\")\n\n\n                                                                                                                                                                                                        postgis_full_version\n1 POSTGIS=\"2.5.2 r17328\" [EXTENSION] PGSQL=\"110\" GEOS=\"3.7.1-CAPI-1.11.1 27a5e771\" PROJ=\"Rel. 5.2.0, September 15th, 2018\" GDAL=\"GDAL 2.3.3, released 2018/12/14\" LIBXML=\"2.9.9\" LIBJSON=\"0.13.1\" LIBPROTOBUF=\"1.3.1\" RASTER\n\nI can also confirm this through a function from the {rpostgis} package.\n\n\npgPostGIS(con)\n\n\n[1] TRUE\n\nI’ll demonstrate two methods to write spatial data. The first writes an ESRI shapefile to a database through the command line. The second directly writes an {sf} object through the st_write() function.\nWriting shapefiles via the command line\nThe first method of writing spatial data to a database does not involve R or RStudio at all. Instead it uses the command line. By opening a terminal window and navigating to the correct directory, I can write a shapefile with the following command. Note that this only works with the shapefile format and not other formats of spatial data.\nshp2pgsql -I -s 4326 -W Latin1 district_shapes.shp district_shapes | psql -d in_household -U postgres\nshp2pgsql is a command line tool that comes with PostgreSQL for converting ESRI shapefiles into SQL suitable for insertion into a PostGIS-enabled PostgreSQL database.\nBreaking down this command, note that:\n-I adds a GiST index on the geometry column\n-s 4326 specifies an SRID (Spatial Reference Identifier)\n-W specifies the encoding\ndistrict_shapes.shp is the file name\ndistrict_shapes is the new table name\n-d in_household is the database name\n-U postgres is the default user\nDirectly writing spatial data with sf::st_write()\nAs in the first method shown for writing non-spatial data, the workflow above includes an intermediary step, in this case, the creation of an ESRI shapefile from the existing {sf} object. This may be useful in some cases, but ESRI shapefiles have certain limitations. For instance, field names greater than ten characters will be automatically abbreviated.\nWe should, however, be able to directly write an {sf} object to the database. I can do this with the st_write() function. Placing the st_write() function inside the purrr::walk() function provides for writing any number of spatial tables in a list or vector.\n\n\nremaining_spatial_tables <- c(\"state_shapes\", \"electricity_latrine_dots\")\n\nwalk(remaining_spatial_tables, ~ {\n    st_write(get(.x), dsn = con, layer = .x, \n             overwrite = FALSE, append = FALSE)\n})\n\n\n\nAs shown below, I can confirm that the spatial data tables have been added with a function from the {rpostgis} package. I can also note that the file written as an ESRI shapefile has a geometry column named “geom”, whereas the objects written with st_write() hold their geometry in a column named “geometry”.\n\n\npgListGeom(con, geog = TRUE)\n\n\n  schema_name               table_name geom_column geometry_type\n1      public          district_shapes        geom  MULTIPOLYGON\n2      public electricity_latrine_dots    geometry      GEOMETRY\n3      public             state_shapes    geometry      GEOMETRY\n      type\n1 GEOMETRY\n2 GEOMETRY\n3 GEOMETRY\n\nQuerying SQL databases from R\nNow that I have imported the requisite tables, I can begin querying the database from R. It is useful to first explore the database with a function like DBI::dbListTables() to see the names of the tables present.\n\n\ndbListTables(con)\n\n\n[1] \"states_abb_region_ut\"     \"country_census\"          \n[3] \"spatial_ref_sys\"          \"electricity_latrine_dots\"\n[5] \"state_shapes\"             \"district_shapes\"         \n[7] \"district_census\"          \"state_census\"            \n\nNote that I did not explicitly create one table in the database, “spatial_ref_sys”. It gets created after installing the PostGIS extension.\nAnother useful {DBI} function is dbListFields() to examine the columns in a specific table by specifiying the connection and the table name.\n\n\ndbListFields(con, \"states_abb_region_ut\")\n\n\n[1] \"state\"     \"abb\"       \"region\"    \"year\"      \"ut_status\"\n\nA similar {rpostgis} function gives a bit more information about the nature of the fields stored in any table.\n\n\ndbTableInfo(con, \"electricity_latrine_dots\")\n\n\n  column_name    data_type is_nullable character_maximum_length\n1     sctl_sc         text         YES                       NA\n2     dm_sctn         text         YES                       NA\n3        year         date         YES                       NA\n4     categry         text         YES                       NA\n5           n      integer         YES                       NA\n6      sum_hh      integer         YES                       NA\n7    geometry USER-DEFINED         YES                       NA\n\nRead an entire table into R\nHaving explored the contents of a database connection, I want to start pulling data from the database into R. To start, I can use the DBI::dbReadTable() function to import an entire remote table into R as a dataframe.\n\n\nstate_census_db <- dbReadTable(con, \"state_census\")\n\nclass(state_census_db)\n\n\n[1] \"data.frame\"\n\nHowever, this is rarely optimal, especially if the tables are large. More often than not, you’ll only want a portion of a table stored in a database. In those situations, you want to query the database to retrieve some specified selection of a table.\nThat can be achieved in a number of different ways, including:\nSQL code inside an R Markdown chunk with an SQL engine\nSQL code inside the DBI::dbGetQuery() function\nreference a dplyr::tbl() and query it with SQL\nreference a dplyr::tbl() and query it with {dplyr} syntax\nSQL inside an R Markdown chunk\nThe first option for querying an SQL database from R includes just writing SQL code inside an R Markdown chunk where the language engine has been changed to SQL, as demonstrated previously. The query below shows the ten states with the lowest rates of electricity access among rural ST households in 2011.\n\nSELECT state, \n      total_hh, \n      num_ea, ROUND(ea::numeric, 2) AS electricity\nFROM state_census\nWHERE year = '2011-01-01' AND\n      societal_section = 'ST' AND\n      demo_section = 'Rural' AND\n      water_source = 'All Sources' AND\n      water_avail = 'Total'\nORDER BY ea\nLIMIT 10;\n\nUsing the output.var chunk option allows me to save the query output into a new dataframe. This however runs into the same problem as dbReadTable() for large datasets.\n\n\nhead(lowest_ea_states)\n\n\n          state total_hh num_ea electricity\n1         Bihar   387584  27164        0.07\n2        Odisha  2090443 263011        0.13\n3 Uttar Pradesh   359499  63928        0.18\n4         Assam   814320 187676        0.23\n5     Jharkhand  1542273 367042        0.24\n6   West Bengal  1116320 287369        0.26\n\nSQL inside the dbGetQuery() function\nAnother approach provides more query flexibility than dbReadTable() while staying within an R chunk to do the job. We can place any SQL query inside the dbGetQuery() function. Here we find the percentage of rural households among South Indian states having access to water within their household premises in 2011.\n\n\nwater_southern_2011 <- dbGetQuery(con,\n           \"SELECT s.state, num_total, num_within, \n                ROUND(within::numeric, 2) AS water_within_home\n            FROM state_census AS s LEFT JOIN states_abb_region_ut AS sabr\n            ON s.state = sabr.state AND\n              s.year = sabr.year\n            WHERE region = 'Southern' AND\n                s.year = '2011-01-01' AND\n                societal_section = 'ALL' AND\n                demo_section = 'Rural' AND\n                water_source = 'All Sources' AND\n                water_avail = 'Total'\n            ORDER BY within DESC;\")\n\nwater_southern_2011\n\n\n           state num_total num_within water_within_home\n1         Kerala   4095674    2984553              0.73\n2     Puducherry     95133      57764              0.61\n3 Andhra Pradesh  14246309    4486906              0.31\n4      Karnataka   7864196    2091969              0.27\n5     Tamil Nadu   9563899    1625884              0.17\n\nNot surprisingly, we can see that Kerala tops the list by a wide margin. Tamil Nadu’s rate is particularly poor.\nThe dbGetQuery() function returns a dataframe for the specified query. However, when dealing with large datasets, this may not always be the best form of output. Sometimes you may want a “lazier” approach.\n\n\nclass(water_southern_2011)\n\n\n[1] \"data.frame\"\n\nReference a dplyr::tbl() and query it with SQL\nWith the addition of the {dbplyr} package, {dplyr} can be used to access data stored on a remote server. It is designed to be as lazy as possible in that it never pulls data into R unless explicitly requested.\nFor example, I can create a reference to a remote table with the tbl() function and execute a query wrapped inside the sql() function. Instead of a dataframe however, this returns a connection object.\nPrinting the object will show the data almost like a normal dataframe. However, it won’t return the number of observations (nrow() fails) because the data is not yet actually in R. The query below finds districts in 2011 with the largest raw difference between rates of household access to electricity and latrines.\n\n\nlargest_gap <- tbl(con, \n                  sql(\"SELECT state, district, ea, la, \n                                ABS(ea - la) AS gap\n                      FROM district_census\n                      WHERE year = '2011-01-01' AND\n                            societal_section = 'ALL' AND\n                            demo_section = 'Total' AND\n                            water_source = 'All Sources' AND\n                            water_avail = 'Total'\n                      ORDER BY gap DESC\"))\n\nclass(largest_gap)\n\n\n[1] \"tbl_PostgreSQLConnection\" \"tbl_dbi\"                 \n[3] \"tbl_sql\"                  \"tbl_lazy\"                \n[5] \"tbl\"                     \n\nLooking at the gap between electricity and latrine rates finds districts all across the country.\n\n\nhead(largest_gap)\n\n\n# Source:   lazy query [?? x 5]\n# Database: postgres 11.4.0\n#   [seanangiolillo@localhost:5432/in_household]\n  state           district      ea    la   gap\n  <chr>           <chr>      <dbl> <dbl> <dbl>\n1 Chhattisgarh    Champa     0.898 0.152 0.746\n2 Chhattisgarh    Kabeerdham 0.852 0.132 0.720\n3 Tamil Nadu      Viluppuram 0.931 0.211 0.719\n4 Tamil Nadu      Ariyalur   0.896 0.181 0.715\n5 Jammu & Kashmir Kathua     0.929 0.221 0.708\n6 Karnataka       Gadag      0.919 0.212 0.707\n\nTo actually import the data into R, after arriving at the correct query, apply the collect() function.\n\n\ncollected_gap <- collect(largest_gap)\nclass(collected_gap)\n\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nnrow(collected_gap)\n\n\n[1] 640\n\nReference a dplyr::tbl() and query it with {dplyr} syntax\nInstead of writing the query as SQL code, I can return the same results with {dplyr} syntax, thanks to {dbplyr}.\n\n\n# still a connection\nmy_gap <- tbl(con, \"district_census\") %>%\n    filter(\n        year == '2011-01-01',\n        societal_section == 'ALL',\n        demo_section == 'Total',\n        water_source == 'All Sources',\n        water_avail == 'Total'\n    ) %>% \n    mutate(gap = abs(ea - la)) %>% \n    select(state, district, ea, la, gap) %>% \n    arrange(desc(gap))\n\nclass(my_gap)\n\n\n[1] \"tbl_PostgreSQLConnection\" \"tbl_dbi\"                 \n[3] \"tbl_sql\"                  \"tbl_lazy\"                \n[5] \"tbl\"                     \n\n{dbplyr} translates {dplyr} code to SQL before sending it to the database. We can actually see the SQL query generated from {dplyr} code using the show_query() function.\n\n\nshow_query(my_gap)\n\n\n<SQL>\nSELECT \"state\", \"district\", \"ea\", \"la\", ABS(\"ea\" - \"la\") AS \"gap\"\nFROM \"district_census\"\nWHERE ((\"year\" = '2011-01-01') AND (\"societal_section\" = 'ALL') AND (\"demo_section\" = 'Total') AND (\"water_source\" = 'All Sources') AND (\"water_avail\" = 'Total'))\nORDER BY \"gap\" DESC\n\nIt’s not as readable as the original SQL code I wrote, but, on pasting it into pgAdmin, I can see that it works. The {dbplyr} vignette notes that it may not be the most natural SQL code for more complicated queries, but, if you don’t know any SQL, it’s a great substitute.\nUse Cases\nThus far, the queries shown have been fairly simple. I now want to generate some more interesting queries.\nMy project exploring how access to electricity covaries with access to latrines revealed a number of districts that fell below the national average in access to electricity, but above the national average in access to latrines. As this is an unusual combination, I want to query the database to find these districts.\nAmong all 2011 households, at the All-India level, 67% and 47% were the national averages for household access to electricity and latrines, respectively, as shown in the query below.\n\n\ndbGetQuery(con, \n            \"SELECT ROUND(ea, 2) AS electricity, \n                    ROUND(la, 2) AS latrines\n            FROM country_census\n            WHERE year = '2011-01-01' AND\n                  societal_section = 'ALL' AND\n                  demo_section = 'Total' AND\n                  water_source = 'All Sources' AND\n                  water_avail = 'Total'\")\n\n\n  electricity latrines\n1        0.67     0.47\n\nNext, with an SQL query in an R Markdown chunk with a SQL engine, I’ll save a view of all districts (matching the same year and other criteria) that fall below the national average in electricity access and above the national average in latrine access.\n\nCREATE OR REPLACE VIEW districts_below_ea_above_la AS\n    SELECT *\n    FROM district_census\n    WHERE year = '2011-01-01' AND\n        societal_section = 'ALL' AND\n        demo_section = 'Total' AND\n        water_source = 'All Sources' AND\n        water_avail = 'Total' AND\n        ea < (\n            SELECT ea\n            FROM country_census\n            WHERE year = '2011-01-01' AND\n            societal_section = 'ALL' AND\n            demo_section = 'Total' AND\n            water_source = 'All Sources' AND\n            water_avail = 'Total'\n        ) AND\n        la > (\n            SELECT la\n            FROM country_census\n            WHERE year = '2011-01-01' AND\n            societal_section = 'ALL' AND\n            demo_section = 'Total' AND\n            water_source = 'All Sources' AND\n            water_avail = 'Total'\n        );\n\nThen in the code below, I join two references to remote tables using the same {dplyr} syntax I would use to join two dataframes. Counting the districts by region reveals that 41 of these 59 regions fall in the Northeastern states.\n\n\ntbl(con, \"districts_below_ea_above_la\") %>% \n    left_join(\n        tbl(con, \"states_abb_region_ut\"), \n        by = c(\"state\", \"year\")\n    ) %>% \n    count(region, sort = TRUE) %>% \n    collect()\n\n\n# A tibble: 4 x 3\n# Groups:   region [4]\n  region       .add      n\n  <chr>        <lgl> <dbl>\n1 Northeastern TRUE     41\n2 Northern     TRUE      9\n3 Eastern      TRUE      7\n4 Western      TRUE      2\n\nFinally, let’s plot these districts on a map.\n\n\n# join region column into selected 59 districts\ntbl(con, \"districts_below_ea_above_la\") %>% \n    left_join(\n        tbl(con, \"states_abb_region_ut\"), \n        by = c(\"state\", \"year\")\n    ) %>% \n    collect() %>% \n    # join in corresponding district shapes\n    left_join(\n        st_read(con, query = \"SELECT * \n                                FROM district_shapes \n                                WHERE year = '2011-01-01';\"),\n        by = c(\"district\", \"state\", \"year\")\n    ) %>% \n    st_as_sf() %>% \n    ggplot() +\n    # add background state shapes\n    geom_sf(\n        data = st_read(con, query = \"SELECT * \n                            FROM state_shapes \n                            WHERE year = '2011-01-01'\")\n        ) +\n    # color 59 districts by region\n    geom_sf(aes(fill = region)) +\n    coord_sf(datum = NA) +\n    labs(\n        title = \"59 districts fall below the national average in access to\\nelectricity and above the national average in access to latrines.\",\n        subtitle = \"41 of these are in the Northeast.\",\n        caption = \"Data refers to all 2011 households\",\n        fill = \"Region\"\n    ) +\n    theme_bw() +\n    theme(\n        plot.title = element_text(size = 11),\n        plot.subtitle = element_text(size = 10)\n    )\n\nggsave(\"img/above_below_map.png\")\n\n\n\n\n\n\nFinal Word and Further Resources\nWhen finished, it’s advised as good practice to formally disconnect from the database. The following command does this.\n\n\ndbDisconnect(con)\n\n\n[1] TRUE\n\nIn this post, I’ve tried to introduce a few different options for the following:\nConnecting to a remote SQL database from R\nWriting data from R to a database\nQuerying data from a database in R\nIn compiling this post, I found all of the resources below to be especially useful:\nCRAN’s manual on R Data Import/Export has a helpful section on relational databases introducing uses for a databse and the RDBMS structure.\nRStudio has a website devoted to using databases with R covering topics like connections, queries, and best practices.\nData Carpentry has a nice tutorial on accessing a database from R and running queries.\nFor a more in-depth look at SQL itself, I found the book Practical SQL by Anthony DeBarros to be a great starting point.\n\n\n\n",
    "preview": "posts/2019-07-02-working-with-databases-in-r/img/featured.png",
    "last_modified": "2020-12-23T09:11:49-05:00",
    "input_file": {},
    "preview_width": 1950,
    "preview_height": 1199
  },
  {
    "path": "posts/2019-05-08-dashboards/",
    "title": "Dashboards with shinydashboard & ggiraph",
    "description": "Explore how access to electricity in India varies with respect to latrine access at state and district levels through a scatterplot, dumbbell plot and a bivariate bubble map.",
    "author": [],
    "date": "2019-05-08",
    "categories": [
      "shiny",
      "shinydashboard",
      "sf",
      "ggiraph",
      "geospatial"
    ],
    "contents": "\n\nContents\nDistrict-level scatterplot across time\nDumbbell plots\nBivariate Bubble Map\n\nThe app linked below uses the {shinydashboard} and {ggiraph} packages to create an interactive visualization of Indian electricity and latrine access data.\nhttps://seanangio.shinyapps.io/shinydash/\nYou can also find its Github repo.\nWhile two previous iterations using this dataset focused on various geospatial representations, this project seeks to shed light on the question of how access to electricity and latrines vary with respect to each other. To what extent do high performers in electricity access also score well in latrine access? How do these metrics vary over time? Across different societal groups? Among urban vs. rural households? At state and district levels? In particular geographic regions?\nThe vastly uneven population counts across states and districts led me to choose visualizations that allow a raw count like population to be mapped to a size aesthetic, specifically a scatterplot, a dumbbell plot, and a bivariate choropleth. Of course, each has their own particular strengths and weaknesses.\nDistrict-level scatterplot across time\nThe scatterplot below is an example of how districts in India have improved their access to electricity and latrines over three decades, but have done so at significantly different rates. It is also interesting to examine the distribution of points. Generally, electricity rates are greater than latrine rates, but this is not exclusively the case.\n\n\n\nMoreover, the dotted lines showing the All-India averages for the selected data divides the plane into four quadrants, identifying states or districts above or below the country average. We might expect to see the top right and bottom left quadrants: places that either do well or do poorly in both electricity and latrine access. The bottom right quadrant is also fairly easy to understand because electricity access tends to exceed latrine access, as demonstrated most vividly by the dumbbell plot. The top left quadrant however, those cases with above average access to latrines but below average access to electricity, are particularly interesting. I do not have any strong intuition for what is happening in this quadrant.\nDumbbell plots\nThe state dumbbell plot below is well suited to showing the gap between a state’s household rate of access to electricity versus its rate of access to latrines. Dotted lines highlight cases where this relationship is reversed, and the rate of latrine access actually exceeds that of electricity.\n\n\n\nThis plot above is busy, but readable at the state level. However, we can dive deeper into any particular state using the geographic filter. The plot below shows the same data for districts in the state of West Bengal.\n\n\n\nBivariate Bubble Map\nPrevious iterations of this project have used choropleths to map the household rate of access to electricity or latrines. However, depending on the underlying data, we would be unable to map both of these metrics simultaneously.\nA bivariate color scheme, however, does enable us to track two variables at the same time. Although we have much less precision in any one category, we can place states or districts into a two-dimensional color bin instead of a typical one-dimensional scale.\n\n\n\nAt the district level, bubbles can become quite crowded, but if we filter to a particular state or region, we can observe the data more clearly, as done here for districts in the Northeast states.\n\n\n\n\n\n\n",
    "preview": "posts/2019-05-08-dashboards/img/featured.png",
    "last_modified": "2020-12-21T21:48:40-05:00",
    "input_file": {},
    "preview_width": 882,
    "preview_height": 672
  },
  {
    "path": "posts/2019-03-25-comparative-thematic-mapping-with-mapdeck/",
    "title": "Comparative Thematic Mapping with Mapdeck",
    "description": "Compare mapping styles like a choropleth, dot density map, proportional symbols map, and 3D choropleth using Indian electricity and latrine access data",
    "author": [],
    "date": "2019-03-25",
    "categories": [
      "shiny",
      "mapdeck",
      "sf",
      "geospatial"
    ],
    "contents": "\n\nContents\nChoropleths\nDot density maps\nProportional symbols maps\n3D choropleths\n\nThe app linked below uses the {mapdeck} library to compare common thematic mapping styles using Indian electricity and latrine access data. These styles include a choropleth, a dot density map, a proportional symbols map, and a 3D choropleth.\nhttps://shiny.socialcops.com/thematic_mapping/\nYou can also find its Github repo.\nA brief summary of pros and cons of these thematic maps is below. For a fuller treatment, please see my SocialCops blog post on this topic.\nChoropleths\nA choropleth gives the simplest view of the spatial distribution of a standardized rate, but conceals the vastly-different underlying population counts.\nChoropleth of Indian household electricity access (1991-2011)\n\n\n\nDot density maps\nA dot density map provides a method to spatially visualize clusters of a raw count, while sacrificing the ability to retrieve numeric data.\nDot density map comparing urban and rural households having access to both electricity and latrines (2011)\n\n\n\nProportional symbols maps\nA proportional symbols map can communicate both a standardized rate and a raw count through the color and size aesthetics, yet congestion is often a dilemma.\nProportional symbols map depicting household latrine access (1991-2011)\n\n\n\n3D choropleths\nA 3D choropleth can map a rate and a raw count, while preserving the original geographic shapes, but at the cost of visual clarity.\n2011 households having access to neither electricity nor a latrine\n\n\n\n\n\n\n",
    "preview": "posts/2019-03-25-comparative-thematic-mapping-with-mapdeck/img/featured.png",
    "last_modified": "2020-12-21T21:50:04-05:00",
    "input_file": {},
    "preview_width": 714,
    "preview_height": 736
  },
  {
    "path": "posts/2019-01-30-animated-and-interactive-maps-in-r/",
    "title": "Animated and Interactive Maps in R",
    "description": "Tutorial for creating animated maps using packages like {tmap} and {gganimate} and interactive maps using packages like {ggiraph}, {mapview}, {leaflet} and {plotly}",
    "author": [],
    "date": "2019-01-30",
    "categories": [
      "tmap",
      "sf",
      "gganimate",
      "mapview",
      "leaflet",
      "plotly",
      "ggiraph",
      "crosstalk",
      "shiny"
    ],
    "contents": "\nIn support of the Social Cops Introduction to GIS in R free online course, I wrote this lesson on creating animated and interactive maps in R. Find it at the link below:\nhttps://socialcops.com/courses/introduction-to-gis-r/lesson4-animated-interactive-maps/\nIt includes many examples like this:\n\n\n\nSource: Social Cops Introduction to GIS in R course\n\n\n\n",
    "preview": "posts/2019-01-30-animated-and-interactive-maps-in-r/img/featured.png",
    "last_modified": "2020-12-16T18:14:13-05:00",
    "input_file": {},
    "preview_width": 596,
    "preview_height": 432
  },
  {
    "path": "posts/2019-01-29-static-maps-in-r/",
    "title": "Static Maps in R",
    "description": "Tutorial for creating static choropleths and cartograms using packages like {tmap}, {ggplot2}, {cartogram}, {geogrid} and {geofacet}",
    "author": [],
    "date": "2019-01-29",
    "categories": [
      "tmap",
      "sf",
      "ggplot2",
      "cartogram",
      "geogrid",
      "geofacet"
    ],
    "contents": "\nIn support of the Social Cops Introduction to GIS in R free online course, I wrote this lesson on creating static maps in R. Find it at the link below:\nhttps://socialcops.com/courses/introduction-to-gis-r/lesson3-static-maps/\n\n\n\n\n\n\n",
    "preview": "posts/2019-01-29-static-maps-in-r/img/featured.png",
    "last_modified": "2020-12-21T21:43:05-05:00",
    "input_file": {},
    "preview_width": 1344,
    "preview_height": 960
  },
  {
    "path": "posts/2019-01-28-spatial-subsetting-in-r/",
    "title": "Spatial Subsetting in R",
    "description": "Learn different topological relations to spatially subset data via the {sf} package",
    "author": [],
    "date": "2019-01-28",
    "categories": [
      "sf",
      "leaflet",
      "ggplot2",
      "tidycensus",
      "tigris",
      "geospatial"
    ],
    "contents": "\nIn support of Lesson 5 of the Social Cops Introduction to GIS in R online course, I created a shiny app to demonstrate the concept of spatial subsetting using median household income data from the Philadelphia metropolitan area. Find the app at the link below.\nhttps://shiny.socialcops.com/spatialsubset/\nThe full lesson on spatial subsetting can be found here.\n\n\n\nYou can also find its Github repo.\n\n\n\n",
    "preview": "posts/2019-01-28-spatial-subsetting-in-r/img/featured.png",
    "last_modified": "2020-12-21T18:48:20-05:00",
    "input_file": {},
    "preview_width": 1425,
    "preview_height": 775
  },
  {
    "path": "posts/2018-12-14-interactive-choropleths-with-shiny-and-leaflet/",
    "title": "Interactive Choropleths with Shiny and Leaflet",
    "description": "Explore electricity, latrine and water access data from the Indian Census",
    "author": [],
    "date": "2018-12-14",
    "categories": [
      "sf",
      "shiny",
      "leaflet",
      "ggplot2",
      "geospatial"
    ],
    "contents": "\nThe app linked below is an interactive visualization of 1991-2011 Indian Census data of Household Access to Electricity, Latrines and Water.\nhttps://shiny.socialcops.com/gisvector/\nA full exploration of the processes behind it can be found on the Social Cops blog here.\n\n\n\nYou can also find its Github repo.\n\n\n\n",
    "preview": "posts/2018-12-14-interactive-choropleths-with-shiny-and-leaflet/img/featured.png",
    "last_modified": "2020-12-16T21:48:28-05:00",
    "input_file": {},
    "preview_width": 1059,
    "preview_height": 708
  },
  {
    "path": "posts/2018-12-13-zoom-triggered-actions-in-leaflet-and-shiny/",
    "title": "Zoom-Triggered Actions in Leaflet and Shiny",
    "description": "Explore median household income data in the Delaware Valley at various levels of scope",
    "author": [],
    "date": "2018-12-13",
    "categories": [
      "sf",
      "leaflet",
      "shiny",
      "ggplot2",
      "tigris",
      "tidycensus",
      "geospatial"
    ],
    "contents": "\nThis is a simple project demonstrating how to trigger actions in Shiny-Leaflet projects using the zoom level. Given a zoom level, the map displays the corresponding state, county or census tract shapes. Find the app at the link below.\nhttps://seanangio.shinyapps.io/dv_income/\n\n\n\nYou can also find its Github repo.\n\n\n\n",
    "preview": "posts/2018-12-13-zoom-triggered-actions-in-leaflet-and-shiny/img/featured.png",
    "last_modified": "2020-12-16T21:50:39-05:00",
    "input_file": {},
    "preview_width": 1128,
    "preview_height": 658
  },
  {
    "path": "posts/2018-08-31-generate-choropleths-and-cartograms-in-shiny/",
    "title": "Generate Choropleths and Cartograms in Shiny",
    "description": "Visualize India's states through a range of geospatial representations",
    "author": [],
    "date": "2018-08-31",
    "categories": [
      "sf",
      "ggiraph",
      "shiny",
      "cartogram",
      "geogrid",
      "geospatial"
    ],
    "contents": "\nIn support of Lesson 4 of the Social Cops Introduction to GIS in R online course, I created a shiny app to allow users to experiment with different geospatial representations, such as choropleths, cartograms and hexbin maps, using basic population and economic data from India’s states. Find the app at the button above or link below.\nhttps://shiny.socialcops.com/indiastates/\nThe full lesson can be found here.\n\n\n\nYou can also find its Github repo.\n\n\n\n",
    "preview": "posts/2018-08-31-generate-choropleths-and-cartograms-in-shiny/img/featured.png",
    "last_modified": "2020-12-21T18:51:50-05:00",
    "input_file": {},
    "preview_width": 807,
    "preview_height": 563
  },
  {
    "path": "posts/2018-08-09-plotting-with-pygal/",
    "title": "Plotting with Pygal",
    "description": "An introduction to Python’s Pygal plotting library",
    "author": [],
    "date": "2018-08-09",
    "categories": [
      "python",
      "pygal"
    ],
    "contents": "\nIn order to brush up on my Python skills, I recently completed the Introduction to Python Scripting Specialization on Coursera from Rice University. Four brief courses introduce Python Programming Essentials, Data Representations, Data Analysis and Data Visualization.\nThe specialization requires writing short Python scripts of a few specified functions, each assessed by a machine grader. The final projects in the Data Visualization course introduce the Pygal plotting library.\nOne of these projects requires unifying World Bank GDP data with Pygal country codes in order to construct a world choropleth for a given year. Below is one example plot output. Although the built-in tooltips are nice (not in the image below), the default color palette makes it difficult to easily distinguish differences in GDP values, even after converting to a log scale.\n\n\n\nThe penultimate project required constructing line plots of World Bank GDP data for a given selection of countries, again using the Pygal plotting library.\n\n\n\nMy scripts for producing these plots can be found in the GitHub repo.\nWhile these scripts recreated the required visualizations, they should not be considered publication-ready. In the map for instance, the legend categorizes the differences between countries having GDP data and those missing, but there is no explanation of what the shades of red represent. The “Missing from World Bank Data” label is also cut-off. Moreover, the tooltips give GDP values with too many decimal points but no sense of units. It also does not utilize space well. The legend could likely fit in the bottom left corner closer to South America, and there could be less space between the title and the map itself.\nThe line plot also suffers from poor formatting of numeric values in the tooltip and y-axis. Placing the legend on the right hand side (ggplot2’s default) would also make better use of space.\n\n\n\n",
    "preview": "posts/2018-08-09-plotting-with-pygal/img/featured.png",
    "last_modified": "2020-12-21T18:53:39-05:00",
    "input_file": {},
    "preview_width": 722,
    "preview_height": 415
  },
  {
    "path": "posts/2018-06-21-mapping-walmarts-growth/",
    "title": "Mapping Walmart's Growth",
    "description": "Recreating a D3 visualization of Walmart’s US growth in R with {gganimate}",
    "author": [],
    "date": "2018-06-21",
    "categories": [
      "gganimate"
    ],
    "contents": "\n\nContents\nReading in the Data\nFirst Plot\nFirst Animation\nLine Plot\nMonthly Growth Animation\nInvestigating High Months\n\nMike Bostock recently posted a D3 visualization of the growth of Walmart locations in the US. I wanted to recreate it in R, animating it with the gganimate package, and do a brief investigation.\n\nN.B.: This post was created with the original gganimate API.\n\nReading in the Data\nThe original D3 visualization can be found here, and the original data he used comes from here.\n\n\nlibrary(tidyverse)\n\ndata <- \"https://gist.githubusercontent.com/mbostock/4330486/raw/fe47cd0f43281cae3283a5b397f8f0118262bf55/walmart.tsv\"\n\nwalmart <- read_csv2(data) %>%\n    separate(col = \"0\\t1\\tdate\", into = c(\"lon\", \"lat\", \"estab\"), sep = \"\\t\") %>%\n    mutate(\n        lon = as.numeric(lon),\n        lat = as.numeric(lat),\n        estab = lubridate::mdy(estab)\n    ) %>%\n    arrange(estab) %>%\n    rowid_to_column(\"number\")\n\n\n\nFirst Plot\nAfter reading in the data, we can make a first plot of all locations with the correct map projection.\n\n\nlibrary(maps)\nlibrary(ggthemes)\nstates <- map_data(\"state\")\n\nggplot() +\n    geom_polygon(data = states, aes(x = long, y = lat, group = group),\n                 fill = \"grey90\", color = \"white\") +\n    coord_map(\"albers\", lat0 = 39, lat1 = 45) +\n    geom_point(data = walmart, aes(x = lon, y = lat), \n               size = 1.5, alpha = 0.8, shape = 1) +\n    theme_map() +\n    ggtitle(\"Walmart's Growth\")\n\n\n\n\n\n\nFirst Animation\nNow that we have a plot that looks quite similar to the final graphic, let’s animate it using the gganimate package. To plot current points in the animation in red, I just plotted the data twice, in one case setting the aesthetic cumulative = TRUE and in the other cumulative = FALSE. We can save the output as a gif.\n\n\nlibrary(animation)\nlibrary(gganimate)\n\nwal_ani <- ggplot() +\n    geom_polygon(data = states, aes(x = long, y = lat, group = group),\n                 fill = \"grey90\", color = \"white\") +\n    coord_map(\"albers\", lat0 = 39, lat1 = 45) +\n    theme_map() +\n    # plot first location\n    geom_point(data = filter(walmart, number == 1), \n               aes(x = lon, y = lat), color = \"blue\", size = 1.5) +\n    # plot all others\n    geom_point(data = filter(walmart, number != 1),\n               aes(x = lon, y = lat, frame = estab, cumulative = TRUE), \n               size = 1.5, alpha = 0.8, shape = 1) +\n    # plot red point when added\n    geom_point(data = filter(walmart, number != 1),\n               aes(x = lon, y = lat, frame = estab, cumulative = FALSE), \n               size = 1.5, color = \"red\") +\n    ggtitle(\"Walmart's Growth\")\n\nani.options(ani.width = 1280, ani.height = 720, interval = 0.1)\ngganimate(wal_ani, filename = \"walmart.gif\")\n\n\n\n\n\n\nIt is interesting to note the strong clustering of locations near the original establishment in Arkansas. Rather than spreading geographically wide, the approach to expansion definitely seems to favor a more gradual regional growth expanding from the origin.\nLine Plot\nA downside of this style of animation is that it can be difficult to discern the pace of the growth. As the plot below shows, it is hardly linear.\n\n\nggplot(walmart, aes(x = estab, y = number)) + \n    geom_line() +\n    scale_y_continuous(\"No. of Locations\", labels = scales::comma) +\n    scale_x_date(\"\") +\n    ggtitle(\"Growth of Walmart Locations Over Time\")\n\n\n\n\n\n\nMonthly Growth Animation\nI can try to account for this in the animation by rounding dates to the nearest month and adding any empty months to the dataset.\n\n\nlibrary(lubridate)\nwalmart <- walmart %>% \n    mutate(rdate = round_date(estab, unit = \"month\")) \n\n# join in empty months\nwalmart_mon <- walmart %>%\n    right_join(\n        as_tibble(seq(min(walmart$rdate), max(walmart$rdate), by = 'month')),\n        by = c(\"rdate\" = \"value\")\n    ) %>%\n    mutate(\n        lon = ifelse(is.na(lon), 0, lon),\n        lat = ifelse(is.na(lat), 0, lat)\n        )\n\n\n\nNow the animation below shows monthly growth, which gives a slightly better impression of the time dimension of the growth.\n\n\nwal_ani_mon <- ggplot() +\n    geom_polygon(data = states, aes(x = long, y = lat, group = group),\n                 fill = \"grey90\", color = \"white\") +\n    coord_map(\"albers\", lat0 = 39, lat1 = 45) +\n    theme_map() +\n    # plot first location\n    geom_point(data = filter(walmart_mon, number == 1), \n               aes(x = lon, y = lat), color = \"blue\", size = 1.5) +\n    # plot all others\n    geom_point(data = filter(walmart_mon, number != 1),\n               aes(x = lon, y = lat, frame = rdate, cumulative = TRUE), \n               size = 1.5, alpha = 0.8, shape = 1) +\n    # plot red point when added\n    geom_point(data = filter(walmart_mon, number != 1),\n               aes(x = lon, y = lat, frame = rdate, cumulative = FALSE), \n               size = 1.5, color = \"red\") +\n    ggtitle(\"Walmart's Monthly Growth\")\n\nani.options(ani.width = 1280, ani.height = 720, interval = 0.1)\ngganimate(wal_ani_mon, filename = \"walmart_mon.gif\")\n\n\n\n\n\n\nInvestigating High Months\nIn the monthly animation, you will notice one especially large flash where a substantial number of locations opened in a single month. In the table below, we can see the months with the greatest number of new locations opening.\nDate\nLocations Opened\nJuly 1981\n88\nFeb 2004\n42\nFeb 2006\n37\nFeb 1991\n36\nFeb 2001\n36\nDid 88 new Walmarts open in July 1981 alone? If we plot Walmart’s growth up to this point, and mark these locations in red, we can see that this month would have been a really big month for the company, particularly making inroads east of Arkansas.\n\n\nggplot() +\n    geom_polygon(data = states, aes(x = long, y = lat, group = group),\n                 fill = \"grey90\", color = \"white\") +\n    coord_map(\"albers\", lat0 = 39, lat1 = 45) +\n    geom_point(data = filter(walmart, rdate < as.Date(\"1981-07-01\")), \n               aes(x = lon, y = lat), \n               size = 1.5, alpha = 0.8, shape = 1) +\n    geom_point(data = filter(walmart, rdate == as.Date(\"1981-07-01\")), \n               aes(x = lon, y = lat), \n               size = 1.5, color = \"red\", shape = 2) +\n    theme_map() +\n    labs(title = \"Walmart's Monthly Growth up to July 1981\", \n         caption = \"July 1981 highlighted in red\")\n\n\n\n\n\n\nHowever, more realistically, this is likely an error in the data. As seen in the plot below, it is such a huge outlier (more than double the next highest month) that a simple error seems to be the most likely explanation.\n\n\nmon_counts <- walmart %>%\n    count(rdate) \n\nmon_counts %>%\n    ggplot(aes(x = rdate, y = n)) + \n    geom_col() +\n    scale_y_continuous(\"No. of Locations\") +\n    scale_x_date(\"\", breaks = seq(as.Date(\"1965-01-01\"), as.Date(\"2005-01-01\"), by = \"5 years\"), labels = scales::date_format(\"%Y\")) +\n    ggrepel::geom_label_repel(\n        data = filter(mon_counts, rdate == as.Date(\"1981-07-01\")),\n        aes(label = format(rdate, \"%b %Y\"))\n    ) +\n    ggtitle(\"Walmart Locations Opened per Month\")\n\n\n\n\n\n\nFor further reference, find this post’s Github repository.\n\n\n\n",
    "preview": "posts/2018-06-21-mapping-walmarts-growth/img/featured.png",
    "last_modified": "2020-12-21T18:59:27-05:00",
    "input_file": {},
    "preview_width": 685,
    "preview_height": 422
  },
  {
    "path": "posts/2018-06-07-text-mining-tf-idf-sentiment-analysis/",
    "title": "Text Mining: TF-IDF & Sentiment Analysis",
    "description": "Tidy text analysis of India PM’s radio addresses in a shiny app",
    "author": [],
    "date": "2018-06-07",
    "categories": [
      "tidytext",
      "shiny",
      "ggplot2",
      "rvest",
      "NLP"
    ],
    "contents": "\nEvery month the Indian Prime Minister Narendra Modi gives a radio address to listeners. I collected those speeches with rvest and created a shiny app to allow users to explore different aspects of the corpus, such as word frequencies, sentiment, and TF-IDF. Find the app at the link below:\nhttps://seanangio.shinyapps.io/mann_ki_baat/\n\n\n\nYou can also find its Github repo.\nRelated to this project, I also wrote a short post introducing the tidytext package found on the Social Cops blog.\n\n\n\n",
    "preview": "posts/2018-06-07-text-mining-tf-idf-sentiment-analysis/img/featured.png",
    "last_modified": "2020-12-21T18:58:19-05:00",
    "input_file": {},
    "preview_width": 362,
    "preview_height": 364
  },
  {
    "path": "posts/2018-05-31-animating-dosas/",
    "title": "Animating Dosas",
    "description": "Scraping Saravana Bhavan’s web site to map restaurant locations in time and space",
    "author": [],
    "date": "2018-05-31",
    "categories": [
      "gganimate",
      "leaflet",
      "tidyverse",
      "rvest"
    ],
    "contents": "\n\nContents\nOverview\nWeb Scraping\nGeocoding locations\nPlot locations on a Leaflet map\nPlot locations over time with gganimate\nPlot growth over time\nConclusion\n\nOverview\nFrom humble origins in Chennai, Saravana Bhavan has grown into a global chain of close to 100 high-quality, vegetarian South Indian restaurants. If you eat at their Delhi location, you’ll find a paper placemat in front of you listing all of their locations around the world. I took the opportunity to scrape their company website for the addresses of these locations and map them.\nThis blog explains how I made a few different visualizations of the chain’s growth, but here is my favorite.\n\n\n\nYou can find the code behind the images in this blog’s Github repo.\nI hope this map, along with a few others I’ve made below, are more informative than the map currently available on Saravana’s website.\n\n\n\n\nN.B. This post was written using the old gganimate API\n\nWeb Scraping\nRather than load all packages at once, I will load them as needed for easier demonstration. I used rvest for web scraping and the tidyverse for data wrangling.\n\n\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(countrycode)\nlibrary(ggmap)\n\n\n\nScrape country pages\nThe first step was to scrape the data from the Saravana Bhavan website. To find the restaurant information, you need to enable Adobe Flash Player to see the map; then choose a country; then a city; and then pick a location. The complete urls contain a country (“cn”), a city (“cy”), and a restaurant id (“rid”). All of these will need to be scraped. An example of this pattern is “http://saravanabhavan.com/restaurants.php?cn=U.A.E&cy=Abu%20Dhabi&rid=73”.\nIn order to do this, I first manually compiled a list of countries, noting their exact spelling and spacing. I then wrote a function to scrape the names of all cities for each country into a dataframe. I also mutated a few columns that will be necessary:\nthe names of cities with spaces replaced by “%20” for functioning urls\nthe urls needed in the next step to scrape city pages\nand a clean version of country names for later plotting\nNext, I loaded in some data from the countrycode package in order to pair each country with its continent. Later we can map this variable to color for some differentiation in our map.\n\n\n# start with vector of countries exactly matching website\ncountries <- c(\"Canada\", \"U.S.A\", \"SouthAfrica\", \"Sweden\", \"Netherland\", \n               \"U.K\", \"Germany\", \"France\", \"SaudiArabia\", \"Bahrain\", \"Qatar\",\n               \"U.A.E\", \"Kuwait\", \"Oman\", \"India\", \"Thailand\", \"HongKong\",\n               \"Malaysia\", \"Singapore\", \"Australia\")\n\n# function to scrape country web pages for city names\nscrape_country <- function(country) {\n    \n    url <- str_c(\"http://www.saravanabhavan.com/restaurants.php?cn=\", country)\n    city <- read_html(url) %>% \n        html_nodes(\".arial-11-gray-bold-black\") %>% \n        html_text() %>%\n        .[!str_detect(., pattern = \"Guest\")]\n\n    tibble(country = country, city = city)\n}\n\n# loop over country web pages to get df of countries and cities, plus complete urls\nhsb <- countries %>%\n    map_dfr(scrape_country) %>%\n    mutate(\n        city_str = str_replace(city, \" \", \"%20\"),\n        city_url = str_c(\"http://www.saravanabhavan.com/restaurants.php?cn=\", \n                         country, \"&cy=\", city_str),\n        clean_country = case_when(\n            country == \"U.S.A\" ~ \"United States\",\n            country == \"U.K\" ~ \"United Kingdom\",\n            country == \"SaudiArabia\" ~ \"Saudi Arabia\",\n            country == \"HongKong\" ~ \"Hong Kong SAR China\",\n            country == \"Netherland\" ~ \"Netherlands\",\n            country == \"U.A.E\" ~ \"United Arab Emirates\",\n            country == \"SouthAfrica\" ~ \"South Africa\",\n            TRUE ~ country\n        )\n    )\n\n# get country-continent pair data from countrycode package for joining\ndata(\"codelist\")\ncontinents <- codelist %>%\n    select(country.name.en, continent)\n\n# fix country names and join with continents\nhsb1 <- hsb %>%\n    left_join(continents, by = c(\"clean_country\" = \"country.name.en\"))\n\n# output file after scraping\nsaveRDS(hsb1, file = \"hsb1.rds\")\n\n\n\nWe can see the results thus far here.\n\n\n\nScrape city pages\nAfter scraping the country pages for city names, we can scrape the city pages for their locations. We can write a very similar function but now target both the name of the location and its corresponding restaurant id (“rid”).\n\n\n# function to get locations and restaurant id from city page\nget_location_rid <- function(city_url) {\n    \n    content <- read_html(city_url)\n    \n    location <- content %>%\n        html_nodes(\"#branchDiv\") %>% \n        html_text(trim = TRUE) %>%\n        str_split(\"\\n\") %>%\n        unlist() %>% \n        str_trim()\n    \n    rid <- content %>%    \n        html_nodes(\".arial-11-gray-bold-black\") %>%\n        html_attr(\"onclick\") %>%\n        str_subset(\"rid=\") %>%\n        word(start = -1, sep = \"=\") %>%\n        word(start = 1, sep = \"'\")\n    \n    tibble(location, rid)\n}\n\n# add locations and rid to each city; mutate complete url for a location\nhsb2 <- hsb1 %>%\n    mutate(location_rid = purrr::map(.$city_url, get_location_rid)) %>%\n    unnest() %>%\n    mutate(rid_url = str_c(city_url, \"&rid=\", rid))\n\n# output another file to avoid scraping again\nsaveRDS(hsb2, file = \"hsb2.rds\")\n\n\n\nAt this point we can look to see which cities and countries have the most locations. Chennai, where the chain started, is of course the clear leader but cities like London, Singapore and Dubai all have a very high number of locations.\n\n\n\nScrape location pages for addresses and launch dates\nI repeated essentially the same process once more, now focused on extracting addresses and launch dates. For addresses, we have to do a bit of cleanup because we extracted more information than we need, such as phone numbers in most cases. These are not always formatted the same way for each page, but luckily address is always listed first. I have also made a few manual adjustments to addresses that are too brief to find a geolocation match. I also took the step of creating a variable for popup text to be displayed on the leaflet map.\n\n\n# function to get address information from location page\nget_address <- function(rid_url) {\n    read_html(rid_url) %>%\n        html_nodes(\".arial-10-gray-bold .arial-10-gray-bold\") %>% \n        html_text(trim = TRUE)\n}\n\n# add addresses for each location to dataframe\nhsb <- hsb %>%\n    mutate(address = purrr::map(.$rid_url, get_address)) %>%\n    unnest() %>%\n    filter(!address == \"View Location Map\")\n\n# remove an erroneous duplicate row\nhsb <- hsb[-c(36),]\n\n# clean addresses by splitting on phone number in most cases\nhsb <- hsb %>%\n    separate(address, into = c(\"address\", \"extra\"),\n             sep = \"Ph:|Ph :|Ph-|Ph -|PH -|Phone:|Tel:|Tel :|Tel  :|Tel /|Phone :|hsb.{1,}@saravanabhavan.com\", remove = FALSE) %>% \n    mutate(address = str_remove(address, \"\\\\.$\"))\n\n# make a few manual adjustments for failing addresses\nhsb$address[hsb$location == \"Vivo City\"] <- str_c(\n    hsb$address[hsb$location == \"Vivo City\"], \", Vivo City, Singapore\")\nhsb$address[hsb$location == \"Bur Dubai\"] <- str_c(\n    hsb$address[hsb$location == \"Bur Dubai\"], \", Bur Dubai, Dubai\")\n\n# concatenate location and address in html for leaflet popups\nhsb <- hsb %>%\n    mutate(\n        popup = str_c(\n            \"<b style='color: blue'>\", location, \"<\/b><br/>\",\n            address\n        )\n    )\n\n\n\nWe can follow the same process for dates. Only two locations are missing launch dates so that will not be a problem, and one of those restaurants missing a launch date looks to be closed anyway.\n\n\n# function to get launch date information from location page\nget_date <- function(rid_url) {\n    \n    launch_date <- read_html(rid_url) %>%\n        html_nodes(\".Arial-11-graybold\") %>% \n        html_text(trim = TRUE) %>%\n        str_split(\": \") %>%\n        unlist %>%\n        .[2]\n    \n    launch_date <- ifelse(is.null(launch_date), NA, launch_date)\n    return(launch_date)\n}\n\n# add addresses for each location to dataframe\nhsb <- hsb %>%\n    mutate(date = purrr::map_chr(.$rid_url, get_date),\n           date = as.Date(date, format = \"%d-%m-%Y\")) \n\n\n\nGeocoding locations\nI used the ggmap package to find longitude and latitude coordinates for each location. Despite being well under a 2,500 query daily limit, I would receive many query limit errors in trying to extract this information, even when adding Sys.sleep() in between calls.\nI was able to overcome this problem using a simplified version of a geocoding function from Shane Lynn. Three locations failed for reasons other than the query limit, and so I just looked them up manually. Be sure to output the object after geocoding completes so to avoid having to do it more than once.\n\n\n# define function for getting coordinates of an address\nget_geo <- function(address) {\n    \n    geo_reply <- geocode(address, output = 'all', \n                         messaging = TRUE, override_limit = TRUE)\n    \n    answer <- tibble(address = address, lat = NA, lon = NA, status = NA)\n    answer$status <- geo_reply$status\n    \n    while (geo_reply$status == \"OVER_QUERY_LIMIT\") {\n        print(\"OVER QUERY LIMIT - Pausing for 1 minute at:\") \n        time <- Sys.time()\n        print(as.character(time))\n        Sys.sleep(60)\n        geo_reply <- geocode(address, output = 'all', messaging = TRUE, override_limit = TRUE)\n        answer$status <- geo_reply$status\n    }\n    \n    if (geo_reply$status != \"OK\") {\n        return(answer)\n    }  else {\n        answer$lat <- geo_reply$results[[1]]$geometry$location$lat\n        answer$lon <- geo_reply$results[[1]]$geometry$location$lng\n        return(answer)\n    }\n    \n}\n\n# get coordinates for each location's address in a new df after removing known failures\naddress_crs <- hsb %>%\n    filter(!location %in% c(\"Southall\", \"Tooting\", \"Al Khuwair\")) %>%\n    .$address %>%\n    map_dfr(get_geo)\n\n# save to rds to avoid re-running geocode\nsaveRDS(address_crs, file = \"address_crs.RDS\")\n\n\n\nLastly, we just need to join the coordinate data back into our main dataset.\n\n\naddress_crs <- readRDS(\"address_crs.RDS\")\n\n# join coordinate data back into main df with 3 manual additions\nfinal_hsb <- hsb %>%\n    left_join(address_crs, by = \"address\") %>%\n    mutate(\n        lat = case_when(\n            location == \"Southall\" ~ 51.5074321,\n            location == \"Tooting\" ~ 51.4299496,\n            location == \"Al Khuwair\" ~ 23.5968027,\n            TRUE ~ lat\n        ),\n        lon = case_when(\n            location == \"Southall\" ~ -0.3800047,\n            location == \"Tooting\" ~ -0.1683871,\n            location == \"Al Khuwair\" ~ 58.4314325,\n            TRUE ~ lon\n        )\n    )\n\n# save final df\nsaveRDS(final_hsb, \"final_hsb.RDS\")\n\n\n\nPlot locations on a Leaflet map\nOnce we have our final dataset we can plot the coordinates on a map. Leaflet is one tool we can use to do this easily. The map below has its pros and cons. It is very easy to interact with as we can zoom in to each cluster and get a better sense of the regional distribution. If we zoom in far enough, we can see exactly where each restaurant is. However, we are probably less interested in each restaurant’s exact location. We could just use Google Maps if we were planning a visit. Instead a better choice might be animating a map that would allow for less interaction but would be able to show the growth of the franchise over time.\n\n\n\nPlot locations over time with gganimate\nWe will be able to tell a better story about the growth of the franchise if we utilize of the launch date variable. Moreover, instead of using exact locations, we can aggregate all locations in one city as a single entity. While we could repeat our geocoding process for city names, it makes more sense to use the coordinates of one of the locations previously found as the coordinates for all locations in that given city. On a world map, we will not be able to notice any intra-city differences anyway.\nLaunch dates were missing for only two restaurants, one of which appears now to be closed, so there is no problem there for our purposes.\n\n\ncity_coordinates <- final_hsb %>%\n    select(city, lat, lon) %>%\n    group_by(city) %>%\n    top_n(1)\n    \n# join coordinate data into df with dates of each location; mutate city totals, labels\nhsb_dates <- final_hsb %>%\n    filter(!is.na(date)) %>%\n    select(city, clean_country, continent, date) %>%\n    mutate(city_country = str_c(city, \", \", clean_country)) %>%\n    left_join(city_coordinates, by = \"city\") %>%\n    group_by(city_country, date, lat, lon, continent) %>%\n    count() %>%\n    arrange(date) %>%\n    group_by(city_country, continent) %>%\n    mutate(city_total = cumsum(n)) %>%\n    ungroup() %>%\n    mutate(\n        hsb_total = cumsum(n),\n        label = str_c(city_country, \" (\", city_total, \")\")\n    )\n\n\n\nAfter the following transformations we can view the growth of the franchise in a table format. The first two decades of existence for the brand is almost exclusively in Chennai and South India.\n\n\n\nBefore animating, we can plot the final map with all locations.\n\n\nlibrary(maps)\nlibrary(ggthemes)\n\nworld <- ggplot() +\n    borders(\"world\", colour = \"gray85\", fill = \"gray80\") +\n    theme_map()\n\nworld +\n    geom_point(data = hsb_dates, \n               mapping = aes(x = lon, y = lat, size = city_total, color = continent), \n               alpha = 0.5) +\n    scale_color_discrete(guide = FALSE) +\n    labs(size = \"No. Restaurants\", \n         title = \"Saravana Bhavan Restaurants Around the World\") +\n    theme(plot.title = element_text(size = 12))\n\n\n\nThe map above however fails to show the growth of the chain over time. That is something we can achieve with a simple animation. With a little guidance from a blog post by Daniela Vázquez, it was fairly easy to make an animated map.\n\n\nlibrary(animation)\nlibrary(gganimate)\n\n# add empty rows for 0 count at beginning and pause at finish\nhsb_dates <- hsb_dates %>%\n    add_row(date = as.Date(\"1981-01-01\"), hsb_total = 0) %>%\n    add_row(date = as.Date(\"2018-05-01\"), hsb_total = 94) %>%\n    arrange(date)\n\nhsb_map <- world +\n    geom_point(data = hsb_dates, mapping = aes(x = lon, y = lat, \n                      size = city_total, color = continent,\n                      frame = date, cumulative = TRUE), \n               alpha = 0.5) +\n    theme(legend.position = \"none\") +\n    labs(size = \"No. Restaurants\", \n         title = \"Growth of Saravana Bhavan Restaurants:\") +\n    ggrepel::geom_label_repel(\n        data = hsb_dates,\n        mapping = aes(x = lon, y = lat, label = label,\n                      frame = date, cumulative = FALSE)\n        ) +\n    geom_label(data = hsb_dates,\n               mapping = aes(label = str_c(\"World Total: \", hsb_total), frame = date,\n                             x = -Inf, y = -88),\n               size = 4, fontface = \"bold\", hjust = 0, vjust = 1, nudge_y = 10) +\n    theme(plot.title = element_text(size = 16))\n\nani.options(ani.width = 640, ani.height = 480, interval = 0.8)\ngganimate(hsb_map, filename = \"hsb_map.gif\")\n\n\n\n\n\n\nPlot growth over time\nThe animated map is excellent at showing the geographical distribution of the chain’s growth. If instead we were more interested in the timeline of the growth, we could better represent the pace of the growth in a lineplot (shown below).\n\n\ndate_seq <- seq(from = as.Date(\"1980/1/1\"), to = as.Date(\"2020/1/1\"), by = \"5 years\")\n\nggplot(hsb_dates, aes(x = date, y = hsb_total)) +\n    geom_point(size = 0.5) +\n    geom_line() +\n    scale_x_date(NULL, breaks = date_seq, date_labels = \"%Y\") +\n    labs(title = \"Establishment of Saravana Bhavan Restaurants\",\n         y = \"No. of Locations\") +\n    theme(plot.title = element_text(size = 12))\n\n\n\nWe could then animate the line, adding the location name for each point.\n\n\nhsb_time <- ggplot(hsb_dates, aes(x = date, y = hsb_total, frame = date)) +\n    geom_point(size = 0.5) +\n    geom_line(aes(cumulative = TRUE)) +\n    ggrepel::geom_label_repel(aes(label = label), \n                              nudge_x = -5, nudge_y = 1, direction = \"both\") +\n    scale_x_date(NULL, breaks = date_seq, date_labels = \"%Y\") +\n    labs(title = \"Establishment of Saravana Bhavan Restaurants:\",\n         y = \"No. of Locations\") +\n    theme(plot.title = element_text(size = 12))\n\nani.options(ani.width = 640, ani.height = 480, interval = 0.8)\ngganimate(hsb_time, filename = \"hsb_time.gif\")\n\n\n\n\n\n\nConclusion\nI hope these examples helped demonstrate the power of packages like rvest for webscraping, leaflet for html maps, and gganimate for animated plots. One possible improvement would be using tweenr to interpolate data between dates in order to smoothen out the transitions from point to point. The most interesting thing I noted about the chain’s growth is the absolute paucity of branches in India, outside of Chennai. Eight locations in London but not a single one in Mumbai, Bangalore, Kolkata, Pune, Ahmedabad, etc? That seems quite odd.\n\n\n\n",
    "preview": "posts/2018-05-31-animating-dosas/img/featured.png",
    "last_modified": "2020-12-21T21:23:57-05:00",
    "input_file": {},
    "preview_width": 803,
    "preview_height": 357
  },
  {
    "path": "posts/2018-05-18-exploring-campaign-contribution-data/",
    "title": "Exploring Campaign Contribution Data",
    "description": "Investigation and visualization of 2016 Presidential election campaign contributions in PA",
    "author": [],
    "date": "2018-05-18",
    "categories": [
      "tidyverse",
      "ggiraph"
    ],
    "contents": "\nFollowing completion of Udacity’s Data Analysis with R course, I conducted an exploratory data analysis of 2016 Presidential campaign contribution data from the state of Pennsylvania. Find it at the link below:\nhttps://rpubs.com/seanangio/PA2016\n\n\n\nYou can also find its Github repo.\n\n\n\n",
    "preview": "posts/2018-05-18-exploring-campaign-contribution-data/img/featured.png",
    "last_modified": "2020-12-21T21:26:50-05:00",
    "input_file": {},
    "preview_width": 530,
    "preview_height": 314
  },
  {
    "path": "posts/2018-03-01-next-word-prediction/",
    "title": "Next Word Prediction",
    "description": "Next word prediction app in support of JHU Data Science Capstone on Coursera",
    "author": [],
    "date": "2018-03-01",
    "categories": [
      "shiny",
      "tidyverse",
      "data.table",
      "NLP"
    ],
    "contents": "\nThe capstone project for the Data Science Specialization on Coursera from Johns Hopkins University is cleaning a large corpus of text and producing an app that generates word predictions based on user input. Find my attempt at the link below:\nhttps://seanangio.shinyapps.io/next_word_app/\n\n\n\nYou can also find its Github repo.\n\n\n\n",
    "preview": "posts/2018-03-01-next-word-prediction/img/featured.png",
    "last_modified": "2020-12-21T21:28:11-05:00",
    "input_file": {},
    "preview_width": 736,
    "preview_height": 397
  },
  {
    "path": "posts/2017-12-17-supervised-learning-exercise-classification/",
    "title": "Supervised Learning: Exercise Classification",
    "description": "Exploring the effectiveness of different ML models to classify motion data into exercise categories",
    "author": [],
    "date": "2017-12-17",
    "categories": [
      "caret",
      "ML"
    ],
    "contents": "\nThe final project for the Practical Machine Learning course of the Data Science Specialization on Coursera from Johns Hopkins University is to use supervised learning techniques to classify repetitions of physical exercises based on the manner in which the motion was completed using accelerometer, gyroscope and magnetometer data.\nFind my attempt at the link below:\nhttps://rpubs.com/seanangio/exercises_classification\n\n\n\nYou can also find its Github repo.\n\n\n\n",
    "preview": "posts/2017-12-17-supervised-learning-exercise-classification/img/featured.png",
    "last_modified": "2020-12-21T21:31:20-05:00",
    "input_file": {},
    "preview_width": 679,
    "preview_height": 472
  },
  {
    "path": "posts/2017-11-23-comparing-fuel-efficiency-via-linear-regression/",
    "title": "Comparing Fuel Efficiency via Linear Regression",
    "description": "Using linear regression models to quantify the difference in fuel efficiency among automatic and manual transmission cars",
    "author": [],
    "date": "2017-11-23",
    "categories": [
      "regression",
      "mtcars"
    ],
    "contents": "\nThe final project for the Linear Regression course of the Data Science Specialization on Coursera from Johns Hopkins University is to assess whether cars with a manual transmission have better fuel efficiency than those with an automatic transmission using only linear regression techniques. The report below attempts to quantify this difference using a variety of linear models.\nhttps://rpubs.com/seanangio/mtcars\n\n\n\nYou can also find its Github repo.\n\n\n\n",
    "preview": "posts/2017-11-23-comparing-fuel-efficiency-via-linear-regression/img/featured.png",
    "last_modified": "2020-12-21T21:32:05-05:00",
    "input_file": {},
    "preview_width": 657,
    "preview_height": 465
  },
  {
    "path": "posts/2017-01-03-predicting-titanic-survival/",
    "title": "Predicting Titanic Survival",
    "description": "My attempt at the classic Kaggle competition to predict survival on the Titanic",
    "author": [],
    "date": "2017-01-03",
    "categories": [
      "caret",
      "ML"
    ],
    "contents": "\nUse the link below to find my report using machine learning workflow to predict survival on the Titanic based on features like age, gender, and passenger class.\nhttps://www.kaggle.com/seanangio/predicting-titanic-survival\n\n\n\nYou can also find its Github repo.\n\n\n\n",
    "preview": "posts/2017-01-03-predicting-titanic-survival/img/featured.png",
    "last_modified": "2020-12-21T21:33:34-05:00",
    "input_file": {},
    "preview_width": 697,
    "preview_height": 498
  }
]
